\documentclass[12pt]{article}

\usepackage[spanish]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{setspace}

\geometry{margin=2.5cm}
\setstretch{1.15}

\title{Proyecto 3\\[0.3em]
Automatización del Proceso de Captura, Ingesta, Procesamiento y Salida de Datos COVID-19 en Colombia}

\author{
Luis Miguel Torres Villegas\\
Jerónimo Acosta Acevedo\\
Juan José Restrepo Higuita
}

\date{
Universidad EAFIT\\
ST0263 -- Sistemas Distribuidos\\
Edwin Nelson Montoya Múnera\\[0.5em]
Semestre 2025-2
}

\begin{document}

\maketitle

\section{Introducción}

Este proyecto implementa un \emph{pipeline} batch de datos para el análisis de casos de COVID-19 en Colombia, utilizando servicios de cómputo distribuido y almacenamiento en la nube sobre Amazon Web Services (AWS). El flujo cubre las etapas de captura, ingesta, procesamiento, generación de resultados analíticos y exposición de estos resultados a través de una API HTTP.

El \emph{dataset} principal corresponde a los casos positivos de COVID-19 reportados en Colombia mediante datos abiertos, complementados con información demográfica y de capacidad hospitalaria simulada, organizada lógicamente como si proviniera de una base de datos relacional.

\section{Objetivos}

\subsection{Objetivo general}

Diseñar e implementar un \emph{pipeline} batch automatizado que tome datos de COVID-19 como fuente principal, los ingrese a un almacenamiento en la nube organizado por zonas, los procese de manera distribuida y produzca salidas refinadas listas para análisis y consumo programático.

\subsection{Objetivos específicos}

\begin{itemize}
  \item Capturar y almacenar el \emph{dataset} de casos de COVID-19 de Colombia en una zona \emph{raw} en Amazon S3.
  \item Emular una fuente relacional (tipo Amazon RDS) mediante archivos CSV con datos demográficos y de capacidad hospitalaria.
  \item Implementar procesos ETL en EMR/Spark para limpiar, normalizar y enriquecer los datos, generando una zona \emph{trusted}.
  \item Calcular métricas agregadas por departamento y generar indicadores en una zona \emph{refined}.
  \item Construir vistas analíticas específicas para consumo por API (resumen nacional diario).
  \item Exponer los resultados a través de Amazon Athena y de un endpoint HTTP basado en AWS Lambda y Amazon API Gateway.
  \item Documentar la arquitectura, las decisiones de diseño y los pasos de ejecución del \emph{pipeline}.
\end{itemize}

\section{Descripción de datos y modelo}

\subsection{Dataset principal de COVID-19}

El \emph{dataset} principal se obtiene de Datos Abiertos Colombia (\texttt{datos.gov.co}):

\begin{itemize}
  \item Nombre: \emph{Casos positivos de COVID-19 en Colombia}.
  \item Identificador: \texttt{gt2j-8ykr}.
  \item Endpoint CSV (Socrata): \url{https://www.datos.gov.co/resource/gt2j-8ykr.csv}.
\end{itemize}

Campos relevantes utilizados en el proyecto:

\begin{itemize}
  \item Fechas:
  \begin{itemize}
    \item \texttt{fecha\_reporte\_web}
    \item \texttt{fecha\_de\_notificaci\_n}
    \item \texttt{fecha\_inicio\_sintomas}
    \item \texttt{fecha\_diagnostico}
  \end{itemize}
  \item Ubicación:
  \begin{itemize}
    \item \texttt{departamento}, \texttt{departamento\_nom}
    \item \texttt{ciudad\_municipio}, \texttt{ciudad\_municipio\_nom}
  \end{itemize}
  \item Características:
  \begin{itemize}
    \item \texttt{edad}, \texttt{unidad\_medida}, \texttt{sexo}
    \item \texttt{fuente\_tipo\_contagio}
  \end{itemize}
  \item Estado:
  \begin{itemize}
    \item \texttt{ubicacion}, \texttt{estado}, \texttt{recuperado}
    \item \texttt{fecha\_muerte}, \texttt{fecha\_recuperado}
  \end{itemize}
\end{itemize}

\subsection{Datos relacionales emulados}

Se define un modelo relacional lógico con dos tablas:

\paragraph{Tabla \texttt{departamento\_demografia}}

\begin{itemize}
  \item \texttt{codigo\_departamento} (PK)
  \item \texttt{nombre\_departamento}
  \item \texttt{region}
  \item \texttt{poblacion}
  \item \texttt{anio\_corte}
\end{itemize}

\paragraph{Tabla \texttt{departamento\_capacidad\_hospitalaria}}

\begin{itemize}
  \item \texttt{id\_capacidad} (PK)
  \item \texttt{fecha}
  \item \texttt{codigo\_departamento} (FK a \texttt{departamento\_demografia})
  \item \texttt{camas\_uci\_totales}
  \item \texttt{camas\_uci\_ocupadas}
  \item \texttt{fuente}
\end{itemize}

Por restricciones de permisos en el entorno de AWS Academy no se crea una instancia real de Amazon RDS, sino que estas tablas se materializan como archivos CSV locales:

\begin{itemize}
  \item \texttt{data/rds/departamento\_demografia.csv}
  \item \texttt{data/rds/departamento\_capacidad\_hospitalaria.csv}
\end{itemize}

Estos archivos se suben a S3 bajo:

\begin{itemize}
  \item \texttt{s3://st0263-proyecto3-covid19/raw/rds/}
\end{itemize}

y son leídos por Spark como extractos de una base de datos relacional.

\subsection{Organización en S3}

El \emph{bucket} principal del proyecto es:

\begin{itemize}
  \item \texttt{st0263-proyecto3-covid19}
\end{itemize}

Se definen tres zonas lógicas:

\begin{itemize}
  \item \textbf{\texttt{raw/}}:
  \begin{itemize}
    \item \texttt{raw/covid/} \textrightarrow{} CSV descargados desde Datos Abiertos.
    \item \texttt{raw/rds/} \textrightarrow{} CSV que emulan las tablas relacionales.
  \end{itemize}
  \item \textbf{\texttt{trusted/}}:
  \begin{itemize}
    \item \texttt{trusted/covid/} \textrightarrow{} casos de COVID limpios y enriquecidos, en formato Parquet particionado por año, mes y día.
    \item \texttt{trusted/demografia/} y \texttt{trusted/capacidad\_hospitalaria/} (planificados para extensión).
  \end{itemize}
  \item \textbf{\texttt{refined/}}:
  \begin{itemize}
    \item \texttt{refined/indicadores\_departamento/} \textrightarrow{} métricas diarias por departamento.
    \item \texttt{refined/api\_views/resumen\_nacional\_diario/} \textrightarrow{} resumen nacional diario para consumo por API.
  \end{itemize}
\end{itemize}

\section{Arquitectura del pipeline}

\subsection{Componentes de AWS utilizados}

\begin{itemize}
  \item \textbf{Amazon S3}: almacenamiento por zonas (\emph{raw}, \emph{trusted}, \emph{refined}).
  \item \textbf{Amazon EMR sobre EC2}: clúster con Apache Spark para procesamiento batch.
  \item \textbf{Amazon Athena}: consultas SQL sobre datos en S3 (zonas \emph{refined}).
  \item \textbf{AWS Lambda}: función \texttt{covid\_resumen\_nacional} que consulta Athena.
  \item \textbf{Amazon API Gateway}: API HTTP que expone el resumen nacional diario.
  \item \textbf{AWS CLI y \texttt{boto3}}: ingesta y automatización desde scripts Python.
\end{itemize}

\subsection{Flujo de datos}

El flujo se organiza en las siguientes etapas:

\subsubsection{Captura e ingesta}

\begin{itemize}
  \item Script \texttt{ingestion/covid\_api/download\_covid\_to\_s3.py}:
  \begin{itemize}
    \item Descarga un CSV del dataset \texttt{gt2j-8ykr} desde Datos Abiertos.
    \item Almacena el archivo en \texttt{raw/covid/} dentro del \emph{bucket} S3.
  \end{itemize}
  \item Script \texttt{ingestion/upload\_rds\_csv\_to\_s3.py}:
  \begin{itemize}
    \item Sube los CSV locales de \texttt{data/rds/} a \texttt{raw/rds/}.
  \end{itemize}
\end{itemize}

\subsubsection{Procesamiento ETL \texttt{raw} \texorpdfstring{$\rightarrow$}{->} \texttt{trusted}}

\begin{itemize}
  \item Script de Spark: \texttt{processing/etl\_trusted/covid\_to\_trusted.py}.
  \item Ejecución en EMR mediante un \emph{step} que corre:
  \begin{itemize}
    \item \texttt{spark-submit s3://st0263-proyecto3-covid19/scripts/etl\_trusted/}\\
        \hspace*{2em}\texttt{covid\_to\_trusted.py}
  \end{itemize}
  \item Funcionalidad principal:
  \begin{itemize}
    \item Lectura de \texttt{raw/covid/*.csv} y \texttt{raw/rds/departamento\_demografia.csv}.
    \item Normalización de nombres de departamento.
    \item \emph{Join} con datos demográficos.
    \item Conversión de fechas y creación de columnas \texttt{anio}, \texttt{mes}, \texttt{dia}.
    \item Escritura de datos limpios y enriquecidos en \texttt{trusted/covid/} en formato Parquet particionado.
  \end{itemize}
\end{itemize}

\subsubsection{Analítica \texttt{trusted} \texorpdfstring{$\rightarrow$}{->} \texttt{refined/indicadores\_departamento}}

\begin{itemize}
  \item Script de Spark: \texttt{processing/analytics\_refined/covid\_indicators\_refined.py}.
  \item \emph{Step} en EMR que ejecuta:
  \begin{itemize}
    \item \texttt{spark-submit s3://st0263-proyecto3-covid19/scripts/analytics\_refined/}\\
        \hspace*{2em}\texttt{covid\_indicators\_refined.py}
  \end{itemize}
  \item Funcionalidad:
  \begin{itemize}
    \item Lectura de \texttt{trusted/covid/}.
    \item Agregación por fecha y departamento para obtener:
    \begin{itemize}
      \item \texttt{casos\_nuevos}
      \item \texttt{casos\_acumulados} (usando una ventana por departamento y fecha)
      \item \texttt{casos\_por\_100k} (cuando hay población disponible)
    \end{itemize}
    \item Escritura en \texttt{refined/indicadores\_departamento/} en formato Parquet particionado.
  \end{itemize}
\end{itemize}

\subsubsection{Vista para API \texttt{refined/indicadores\_departamento} \texorpdfstring{$\rightarrow$}{->} \texttt{refined/api\_views}}

\begin{itemize}
  \item Script de Spark: \texttt{processing/api\_views/resumen\_nacional\_diario.py}.
  \item \emph{Step} en EMR que ejecuta:
  \begin{itemize}
    \item \texttt{spark-submit s3://st0263-proyecto3-covid19/scripts/api\_views/}\\
        \hspace*{2em}\texttt{resumen\_nacional\_diario.py}
  \end{itemize}
  \item Funcionalidad:
  \begin{itemize}
    \item Agregación de indicadores por departamento a nivel nacional para cada fecha.
    \item Cálculo de:
    \begin{itemize}
      \item \texttt{casos\_nuevos\_nacional}
      \item \texttt{casos\_acumulados\_nacional}
      \item \texttt{poblacion\_total\_aprox}
      \item \texttt{casos\_por\_100k\_nacional}
    \end{itemize}
    \item Escritura de la vista en \texttt{refined/api\_views/resumen\_nacional\_diario/} en formato Parquet particionado.
  \end{itemize}
\end{itemize}

\subsection{Capa de consumo: Athena y API}

\subsubsection{Athena}

\begin{itemize}
  \item Base de datos: \texttt{covid\_analytics}.
  \item Tablas externas definidas:
  \begin{itemize}
    \item \texttt{indicadores\_departamento} sobre \texttt{refined/indicadores\_departamento/}.
    \item \texttt{resumen\_nacional\_diario} sobre \texttt{refined/api\_views/resumen\_nacional\_diario/}.
  \end{itemize}
  \item Particiones registradas mediante:
  \begin{itemize}
    \item \texttt{MSCK REPAIR TABLE indicadores\_departamento;}
    \item \texttt{MSCK REPAIR TABLE resumen\_nacional\_diario;}
  \end{itemize}
\end{itemize}

\subsubsection{Lambda y API Gateway}

\paragraph{Lambda}

\begin{itemize}
  \item Función: \texttt{covid\_resumen\_nacional}.
  \item Rol de ejecución: \texttt{LabRole} del entorno del laboratorio.
  \item Lógica:
  \begin{itemize}
    \item Recibe eventos con \texttt{queryStringParameters}.
    \item Si se indica \texttt{fecha=YYYY-MM-DD}, consulta en Athena la tabla \texttt{resumen\_nacional\_diario} para esa fecha.
    \item Si no se indica fecha, obtiene la última fecha disponible.
    \item Devuelve un JSON con:
    \begin{itemize}
      \item \texttt{fecha}
      \item \texttt{casos\_nuevos\_nacional}
      \item \texttt{casos\_acumulados\_nacional}
      \item \texttt{poblacion\_total\_aprox}
      \item \texttt{casos\_por\_100k\_nacional}
    \end{itemize}
  \end{itemize}
\end{itemize}

\paragraph{API Gateway}

\begin{itemize}
  \item Tipo: HTTP API.
  \item Nombre: \texttt{st0263-proyecto3-covid-api}.
  \item \emph{Stage}: \texttt{prod}.
  \item URL base:
  \begin{itemize}
    \item \url{https://n0znyvjr0k.execute-api.us-east-1.amazonaws.com/prod}
  \end{itemize}
  \item Rutas:
  \begin{itemize}
    \item \texttt{GET /resumen-nacional}
    \item \texttt{GET /resumen-nacional?fecha=YYYY-MM-DD}
  \end{itemize}
\end{itemize}

\section{Ejecución del pipeline}

La ejecución completa del flujo, desde la captura hasta el consumo, se resume en los siguientes pasos:

\begin{enumerate}
  \item Preparar el entorno local:
  \begin{enumerate}
    \item Clonar el repositorio del proyecto.
    \item Crear y activar un entorno virtual de Python.
    \item Instalar \texttt{requests} y \texttt{boto3}.
    \item Configurar credenciales con \texttt{aws configure}.
  \end{enumerate}
  \item Ejecutar la ingesta:
  \begin{enumerate}
    \item \texttt{python ingestion/covid\_api/download\_covid\_to\_s3.py}
    \item \texttt{python ingestion/upload\_rds\_csv\_to\_s3.py}
  \end{enumerate}
  \item Crear el clúster EMR y lanzar los \emph{steps} de Spark:
  \begin{enumerate}
    \item ETL \texttt{raw} $\rightarrow$ \texttt{trusted} con \texttt{covid\_to\_trusted.py}.
    \item \texttt{trusted} $\rightarrow$ \texttt{refined/indicadores\_departamento} con \texttt{covid\_indicators\_refined.py}.
    \item \texttt{refined/indicadores\_departamento} $\rightarrow$ \texttt{refined/api\_views/resumen\_nacional\_diario} con \texttt{resumen\_nacional\_diario.py}.
  \end{enumerate}
  \item Actualizar las particiones de las tablas externas en Athena.
  \item Consumir los resultados:
  \begin{enumerate}
    \item Ejecutar consultas SQL en Athena.
    \item Consumir la API HTTP:
    \begin{itemize}
      \item \texttt{GET /resumen-nacional}
      \item \texttt{GET /resumen-nacional?fecha=YYYY-MM-DD}
    \end{itemize}
  \end{enumerate}
\end{enumerate}

\section{Limitaciones y adaptaciones al entorno}

\begin{itemize}
  \item No se crea una instancia real de Amazon RDS debido a restricciones IAM (\texttt{rds:CreateDBInstance} no permitido). Se emula la fuente relacional mediante CSV en \texttt{raw/rds/}, integrados desde Spark.
  \item La creación de roles IAM personalizados está limitada. Se reutiliza \texttt{LabRole} como rol de ejecución para Lambda.
  \item La orquestación del \emph{pipeline} se realiza mediante ejecución manual de \emph{steps} de EMR. En un entorno productivo podría automatizarse con servicios adicionales (por ejemplo, Step Functions o herramientas externas de orquestación), que aquí no se utilizan para ajustarse al entorno del laboratorio.
\end{itemize}

\section{Resultados}

\begin{itemize}
  \item Datos de COVID-19 cargados en S3 en la zona \texttt{raw/covid}.
  \item Datos demográficos y de capacidad hospitalaria cargados en \texttt{raw/rds}.
  \item Zona \texttt{trusted/covid} con datos limpios y enriquecidos, particionados por fecha.
  \item Zona \texttt{refined/indicadores\_departamento} con indicadores diarios por departamento.
  \item Vista \texttt{refined/api\_views/resumen\_nacional\_diario} con el resumen nacional diario.
  \item Tablas externas en Athena que permiten analizar los indicadores por departamento, región y país, así como el comportamiento agregado nacional.
  \item API HTTP operativa que devuelve un resumen nacional diario en formato JSON, listo para ser consumido por aplicaciones cliente o paneles de visualización.
\end{itemize}

\section{Conclusiones}

\begin{itemize}
  \item Es posible construir un \emph{pipeline} batch completo sobre AWS combinando S3, EMR, Athena, Lambda y API Gateway, incluso con restricciones de permisos, siempre que se adapten las fuentes de datos y la forma de integración.
  \item La separación en zonas \emph{raw}, \emph{trusted} y \emph{refined} facilita la trazabilidad, permite reprocesos y clarifica las responsabilidades de cada etapa del flujo de datos.
  \item El uso de Spark en EMR permite procesar volúmenes grandes de información de forma distribuida, aplicar transformaciones complejas y generar indicadores analíticos en formatos eficientes (Parquet).
  \item Athena ofrece una capa de consulta flexible sobre S3 sin administrar servidores adicionales, lo que simplifica la exposición de datos tanto para usuarios SQL como para servicios \emph{serverless}.
  \item La combinación de Lambda y API Gateway permite exponer vistas analíticas específicas como servicios HTTP, acercando el \emph{pipeline} de datos a aplicaciones externas y facilitando la integración con otros sistemas.
\end{itemize}

\end{document}
